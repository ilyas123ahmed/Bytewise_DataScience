# Spiders

### How does its work?
- Spider goes into the source code of website, then it searches for h2 tag and finds out the data and stores in database.
- Find the URL that you want to scrape
- Inspecting the Page
- Find the data you want to extract
- Write the code
- Run the code and extract the data
- Store the data in the required format

### Usages?
Scrapy is a popular web scraping framework we can use to develop scalable scrapers and crawlers.As a web scraping tool, Scrapy has support for proxies, and you will most likely make use of proxies in our scraping project.

# robots.txt
What is robots.txt in web scraping?
A robots.txt file tells search engine crawlers which URLs the crawler can access on your site. This is used mainly to avoid overloading your site with requests; it is not a mechanism for keeping a web page out of Google. To keep a web page out of Google, block indexing with noindex or password-protect the page.
